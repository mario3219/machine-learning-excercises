{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZgHjPlk9tuSf"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.signal import chirp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zmz2obsfs16"
      },
      "source": [
        "**Sound data**\n",
        "\n",
        "Sound will normally be time series data, but can also be supplied in different transformations. Here we will look at a simple example of how a chirp signal looks as time series, as a spectrogram and how you could make features from the energies in areas of the spectrogram.\n",
        "\n",
        "You are not expected to know this, it is just to give a background to the data we will work with in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo_KzoQLfsDi"
      },
      "outputs": [],
      "source": [
        "# Generate timeseries of a chirp\n",
        "fs = 100\n",
        "t = np.arange(0, 10, 1/fs)\n",
        "y = chirp(t, 1, 10, 10, method=\"linear\")\n",
        "plt.plot(t, y)\n",
        "plt.xlabel(\"time\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS8Aa0E2hJpC"
      },
      "outputs": [],
      "source": [
        "# Generate a spectrogram of the chirp\n",
        "spec, freq, ts, im = plt.specgram(y, Fs=fs)\n",
        "plt.xlabel(\"time\")\n",
        "plt.ylabel(\"frequency\")\n",
        "\n",
        "# Plot a box in which we can sum up the energy\n",
        "plt.plot([ts[3], ts[5]], [6, 6], 'r')\n",
        "plt.plot([ts[3], ts[5]], [8, 8], 'r')\n",
        "plt.plot([ts[3], ts[3]], [6, 8], 'r')\n",
        "plt.plot([ts[5], ts[5]], [6, 8], 'r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjERUmc0no6J"
      },
      "source": [
        "We can integrate up the energy over certain frequency bands and time periods, this integrated value is then used as a feature. So the red rectangle here could for example be the source of one of many features from this spectrogram, probably one with a relatively high value given that it hits the high energy band created by the chirp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDbHf6n-grAN"
      },
      "source": [
        "**Sonar Dataset**\n",
        "\n",
        "This dataset consists of sonar chirps bouncing off different surfaces. The chirp is sent out and bounces on either a mine (metal cylinder) or a rock and the returning sound is then recorded. The features in the dataset represents the energy within a particular frequency band, integrated over a certain period of time for signals recorded from different aspect angles.\n",
        "\n",
        "Manual feature extraction like this is very common in many fields, and can make learning much faster and easier by giving the model features containing important information for the task we want performed so it does not have to find that through learning.\n",
        "\n",
        "There is one catch though, when simplifying the data like this we can also remove information contained in the original data. The model do get some new information in that some human thought this transform might be good, and it does not have to try to learn from the complex raw sound, but as with almost everything there is a tradeoff with manually creating features. A sufficiently complex model could find any of these transformation by it self given enough data, and it might also find more efficient transformations.\n",
        "\n",
        "In this case we do not have a lot of data, so doing a this transformation to extract features could be a good idea if the features are good. This will allow simpler models to be used, and can speed up learning and prediction accuracy for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dg1JnKZtoXa"
      },
      "outputs": [],
      "source": [
        "# Download dataset from url, save it to sonar.csv\n",
        "!wget -O sonar.csv https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BStGbJPXuXEL"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"sonar.csv\", header=None)\n",
        "data.iloc[:, 60] = 1 * (data.iloc[:, 60] == 'M') # Replace labels M and R with 1 and 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikzMNAHAvJ57"
      },
      "source": [
        "**Exploring the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4QqFKM8vOeR"
      },
      "outputs": [],
      "source": [
        "# Inspect 5 random samples\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdlcnNyWoIs9"
      },
      "source": [
        "**Histograms**\n",
        "\n",
        "In pandas you can plot histograms over all features with `data.hist()` as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzyzR1vIrXcW"
      },
      "outputs": [],
      "source": [
        "data.hist(figsize=(12, 12));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA8FgDQ3yxGE"
      },
      "source": [
        "Your task is to create your own version of this where the histogram is colored according to what label the contributing datapoints have, the labels are in the last column.\n",
        "\n",
        "Then take a look at [docs](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html) for pyplot histograms and especially look at the stacked keyword.\n",
        "\n",
        "Figure out how to split up all values of a feature corresponding according to the label 0 (rock) or 1 (mine), and then use `hist` to plot them stacked on eachother to look something like\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Create a figure with subplots in a grid and loop over them and plot a histogram of corresponding feature in each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3YAWoYJfyJF"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(8, 8, figsize=(15, 15))\n",
        "for i in range(60):\n",
        "    ax = axes[i // 8, i % 8]\n",
        "    # column_i_label_0 = ...\n",
        "    # column_i_label_1 = ...\n",
        "    # ax.hist(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls7_fHJq0AT7"
      },
      "source": [
        "**Scatter Matrix**\n",
        "\n",
        "Pandas also has a plotting tool called `scatter_matrix` which can give good insight in how the data is spread for combinations of features. We can also supply colors to use for each datapoint, and with that we can create scatterplots over how the labels are distributed for combinations of features. This is very similar to what we just created in 5.1, but for combinations of features instead of for evey feature by it self.\n",
        "\n",
        "Using either this or your plot from previous task and find maybe 5 features that seem like they contain information for classification. Try the first classification task for both all features and your selected features and see if you managed to choose good features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neWJLl2ccDOy"
      },
      "outputs": [],
      "source": [
        "cols = list(map(lambda x: \"red\" if x == 0 else \"blue\", data.iloc[:,60]))\n",
        "somefeatures = range(0, 6) # change here to study other features\n",
        "pd.plotting.scatter_matrix(data.iloc[:,somefeatures], c=cols, figsize=(12, 12));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKwZq_k75kkZ"
      },
      "source": [
        "**Feature selection**\n",
        "\n",
        "Here you can select which features to use, test the classification task below both with all features and with some subset you selected. For the remaining task you can use the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9DT1gdXvUTY"
      },
      "outputs": [],
      "source": [
        "feature_selection = range(0, 60) # All features\n",
        "#feature_selection = [1, 2, 3, 4, 5] # Only a few features\n",
        "\n",
        "# Split data into features and labels, convert labels R->0 and M->1\n",
        "X, y = data.iloc[:, feature_selection], data.iloc[:, 60]\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxR03Ruh9Ees"
      },
      "source": [
        "**Classification**\n",
        "\n",
        "We now test two simple classifiers, kNN and Decision Trees, for the selected data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZMWyO2DtoLT"
      },
      "outputs": [],
      "source": [
        "# To have consistent performance for decision trees here and for normalized version\n",
        "# we need the same random state, and since I say it should not matter with scaling\n",
        "# for decision trees I think it is nicer to get the same result.\n",
        "np.random.seed(37)\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    DecisionTreeClassifier(max_depth=3)]\n",
        "\n",
        "for clf in classifiers:\n",
        "    clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    print(\"{} test accuracy: {}\".format(clf.__class__.__name__, score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDuDu382ZOqN"
      },
      "source": [
        "**Normalization**\n",
        "\n",
        "Normalizing input is something that can have a huge impact for some classification methods. For decision trees it will not make a difference, but for kNN it can affect the outcome by quite a bit. Let's try the exact same but with a rescaler in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHt0ol-oZpc8"
      },
      "outputs": [],
      "source": [
        "np.random.seed(37)\n",
        "\n",
        "pipelines = [Pipeline([('scaler', StandardScaler()), ('clf', clone(clf))]) for clf in classifiers]\n",
        "\n",
        "for pipeline in pipelines:\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    score = pipeline.score(X_test, y_test)\n",
        "    print(\"Scaled {} test accuracy: {}\".format(pipeline['clf'].__class__.__name__, score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgL74K8QxRAk"
      },
      "source": [
        "**Ensambles**\n",
        "\n",
        "Bagging is a type of ensable algorithm where we create random subsets of the data and use them for training multiple different classifiers. Bootstrapping is the part of bagging where we resample uniformly with replacement, i.e. the same data can be sampled multiple times.\n",
        "\n",
        "As discussed in the lecture bagging will help reduce variance, but not bias, and will show the greatest effect for unstable classifiers such as decision trees compared to more stable ones such as kNN. So if we pick classifiers with little to no bias, but high variance, and just train a bunch of them we can hopefully achieve both low variance and low bias.\n",
        "\n",
        "Using bagging on decision trees we get what is known as a random forest, a bunch of trees trained on different subsets of the data and features, and maybe with different parameter settings to increase variance among them.\n",
        "\n",
        "Create a `RandomForestClassifier` and try to tune the hyperparameters so you at least get it better than a single decision tree, but maybe also close to what the kNN classifier can acheive? [Here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) you can find the docs for a `RandomForestClassifier`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSaN6XaB72N0"
      },
      "outputs": [],
      "source": [
        "# Create a random forest classifier, train it on training data and check its score on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJRP3GkTbIO1"
      },
      "source": [
        "**Grid search**\n",
        "\n",
        "As you might have experienced above it is not always easy to find good parameters by hand, and as the problem gets more complex you will have even more parameters to tune. Instead it can be useful to automate this process and make the computer find the some good hyperparameters for you.\n",
        "\n",
        "Grid search will try all possible combinations of parameters you supply to it and see which one gives the best cross validation score. When fitting the data does not take long time it can be a very useful tool, but the time it takes will grow quickly with the number of parameters and the possible variations of the parameters. For more complex problems there faster methods to search for hyperparameters, but we won't go into them now.\n",
        "\n",
        "This one might take a minute or two to run the way we supplied it. We encourage you to change the grid if you want, just be aware of the time it will take with increasing amounts of gridpoints to search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVFTLth-_0Em"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 1000],\n",
        "    'max_samples': [0.7, 0.9],\n",
        "    'max_features': [0.1, 0.3],\n",
        "    'bootstrap': [True, False]\n",
        "    }\n",
        "\n",
        "search = GridSearchCV(clf, param_grid, cv=5, verbose=1)\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "score = search.score(X_test, y_test)\n",
        "print(\"Best CV score: {} using {}\".format(search.best_score_, search.best_params_))\n",
        "print(\"Test accuracy: {}\".format(score))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ex5_bagging.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
