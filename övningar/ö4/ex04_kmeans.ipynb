{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq5bOF9cp2PT"
      },
      "source": [
        "The code illustrates how Kmeans can be used as a preprocessing step to improve performance of a classifier of images of digits 0-9.\n",
        "There are 1797 images of size 8x8, each flattened to a vector of size 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5HcmuocV9T_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline\n",
        "X_digits, y_digits = load_digits(return_X_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLvpkouOqq-L"
      },
      "source": [
        "The images are transformed into a n-dimensional space (try n=10, 15 and 20), described by the distances to the cluster centers.\n",
        "Note that the original dimension is 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr_hw0qvfdAg"
      },
      "outputs": [],
      "source": [
        "improvement = np.array([])\n",
        "n_clusters = 20  # It would seem natural to choose 10 clusters, but some numbers can be written in different ways...\n",
        "\n",
        "for random_state in range(0,10):   # since Kmeans performance is random, we average over some trials\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=random_state)\n",
        "\n",
        "  # Direct classification using if Logistic Regression\n",
        "  log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=random_state)\n",
        "  log_reg.fit(X_train, y_train)\n",
        "  before = log_reg.score(X_test, y_test)\n",
        "\n",
        "  # Use k-means before classifying with Logistic Regression\n",
        "  pipeline = Pipeline([\n",
        "    (\"kmeans\", KMeans(n_clusters=n_clusters, random_state=random_state)),\n",
        "    (\"logreg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=random_state)),])\n",
        "  pipeline.fit(X_train, y_train)\n",
        "  after = pipeline.score(X_test, y_test)\n",
        "\n",
        "  # Compare the results\n",
        "  print('test {} improved accuracy by {:.4f}'.format(random_state,after-before))\n",
        "  print('     accuracy is now {:.4f}'.format(after))\n",
        "  improvement = np.append(improvement,after-before)\n",
        "\n",
        "print('average improvement = {:.4f}'.format(np.mean(improvement)))\n",
        "# Note: The improvement for 8*8 images is not very large, close to zero"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}